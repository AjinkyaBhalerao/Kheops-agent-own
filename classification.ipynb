{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "epD_j_HPZ3vi"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import re\n",
        "from pdfminer.high_level import extract_pages\n",
        "from pdfminer.layout import *\n",
        "import textwrap\n",
        "import math\n",
        "import json\n",
        "import sys\n",
        "\n",
        "pdf_path = \"civil-code.pdf\"\n",
        "starting_words = [\"Titre\", \"Title\", \"Chapitre\", \"Chapter\", \"Section\", \"Sous-section\",\"Sub-section\", \"Paragraphe\",\"Paragraph\" ,\"Article\", \"Livre\", \"Book\" ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4YIyr6_aWODt"
      },
      "outputs": [],
      "source": [
        "def get_footers(fitz_doc):\n",
        "    footers = []\n",
        "    for page in fitz_doc:\n",
        "        data = page.get_text(\"blocks\")\n",
        "        h = page.rect.height\n",
        "\n",
        "        for l in data:\n",
        "            if l[1] > 0.90*h:\n",
        "                text = l[4]\n",
        "                footers.append(text[:-1]) # remove the newline character at end of footer\n",
        "\n",
        "    return footers\n",
        "\n",
        "def extract_sentences_with_starting_words(pdf_path, starting_words):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    word_sentences = [[] for _ in range(len(starting_words))]\n",
        "    current_word = None\n",
        "    previous_line_empty = False\n",
        "    previous_line_word = None\n",
        "\n",
        "    # Get footers to check later if line is a footer\n",
        "    footers = get_footers(doc)\n",
        "\n",
        "    for page in doc:\n",
        "        text = page.get_text()\n",
        "        lines = text.splitlines()\n",
        "        for line in lines:\n",
        "            if line.strip():  # Check if the line is not empty\n",
        "                # Check if the line starts with any of the starting words\n",
        "                for i, word in enumerate(starting_words):\n",
        "                    if line.lower().startswith(word.lower()) and line[0].isupper():\n",
        "                        current_word = i\n",
        "                        word_sentences[current_word].append(line.strip())\n",
        "                        previous_line_word = i\n",
        "                        break\n",
        "                else:\n",
        "                    # Append the line to the current sentence if a starting word is detected in the previous line\n",
        "                    if current_word is not None and line not in footers:\n",
        "                        word_sentences[current_word][-1] += ' ' + line.strip()\n",
        "                        previous_line_word = current_word\n",
        "            elif previous_line_word is not None and not previous_line_empty:\n",
        "                # End the sentence if the current line is empty and the previous line wasn't empty\n",
        "                current_word = None\n",
        "                previous_line_word = None\n",
        "\n",
        "            previous_line_empty = not line.strip()  # Update the flag for the previous line\n",
        "\n",
        "    doc.close()\n",
        "    return word_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def concat_strings(input_list):\n",
        "    result_list = []\n",
        "    temp_string = \"\"\n",
        "    prev_attributes = None\n",
        "\n",
        "    for item in input_list:\n",
        "        text, font, size, float_val = item\n",
        "        attributes = (font, size, float_val)\n",
        "\n",
        "        if attributes == prev_attributes: # and not re.search(r'Bold', font, re.IGNORECASE):\n",
        "            if text[0].isupper() and re.search(r'Bold', font, re.IGNORECASE):\n",
        "                temp_string = text\n",
        "            else:\n",
        "                temp_string += \" \" + text\n",
        "        else:\n",
        "            if temp_string:\n",
        "                result_list.append([temp_string.strip()] + list(prev_attributes))\n",
        "            temp_string = text\n",
        "            prev_attributes = attributes\n",
        "\n",
        "    # Append the last concatenated string\n",
        "    if temp_string:\n",
        "        result_list.append([temp_string.strip()] + list(prev_attributes))\n",
        "\n",
        "    return result_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_text_by_fontsize(pdf_url):\n",
        "    extracted_text = \"\"\n",
        "    curr_font = None\n",
        "    curr_size = None\n",
        "    font_attr = []\n",
        "\n",
        "    for page_layout in extract_pages(pdf_url):\n",
        "        for element in page_layout:\n",
        "            for line in element:\n",
        "                thisline = []\n",
        "                if isinstance(line, LTTextLine):\n",
        "                    for char in line:\n",
        "                        if isinstance(char, LTChar):\n",
        "                            ft = char.fontname\n",
        "                            sz = math.ceil(char.size)\n",
        "                            x = char.bbox[0]\n",
        "\n",
        "                            #if ft != curr_font or sz != curr_size:\n",
        "\n",
        "                            l = line.get_text()\n",
        "                            thisline.append(l[:-1])\n",
        "                            thisline.append(ft)\n",
        "                            thisline.append(sz)\n",
        "                            thisline.append(x)\n",
        "                            font_attr.append(thisline)        \n",
        "                        break\n",
        "\n",
        "    result_list = concat_strings(font_attr)\n",
        "    return result_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_text(font_name, font_size, x):\n",
        "  # Classification rules here based on font_name, font_size, and y (position)\n",
        "  if re.search(r'Bold', font_name, re.IGNORECASE) and (font_size > 14 or x > 100):  # Threshold based on PDF\n",
        "    return \"Titre\"\n",
        "  elif re.search(r'Bold', font_name, re.IGNORECASE) and font_size > 13:\n",
        "    return \"Section\"\n",
        "  elif font_size > 8:\n",
        "    return \"Paragraphe\"\n",
        "  else:\n",
        "    return \"Footer\"\n",
        "\n",
        "\n",
        "def process_pdf(pdf_text):\n",
        "  l2 = []\n",
        "  for line_attr in pdf_text:\n",
        "    \n",
        "    text = line_attr[0]\n",
        "    font_name = line_attr[1]\n",
        "    font_size = line_attr[2]\n",
        "    x = line_attr[3]\n",
        "    category = classify_text(font_name, font_size, x)\n",
        "    # Do something with the classified text and category\n",
        "    #print(f\"{text}, Category: {category}\")\n",
        "    map = (category, text)\n",
        "    l2.append(map)\n",
        "  return l2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sTkjJ0rmgodr"
      },
      "outputs": [],
      "source": [
        "def comp(l1, l2):\n",
        "    # Convert list1 to a set of sentences for efficient comparison\n",
        "    set1 = set([(category, sentence) for category, sentence in l1])\n",
        "\n",
        "    combined_sentences = l1[:]\n",
        "    \n",
        "    temp = []\n",
        "    for c, s in l1:\n",
        "        temp.append(s)\n",
        "    \n",
        "    for category, sentence in l2:\n",
        "    \n",
        "        if sentence not in temp:\n",
        "            combined_sentences.append((category, sentence))\n",
        "\n",
        "    def sort_key(item):\n",
        "    # Get the first key in the dictionary and return it\n",
        "        return next(iter(item))\n",
        "\n",
        "    # Sort the list of dictionaries\n",
        "    sorted_data = sorted(combined_sentences, key=sort_key)\n",
        "    \n",
        "    return sorted_data    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_map(suchi):\n",
        "    naksha = {}\n",
        "    for i, (c, s)  in enumerate(suchi):\n",
        "        naksha[i] = {\"Category\": c, \"Text\":s}\n",
        "\n",
        "    unique_map = {}\n",
        "    for key, value in naksha.items():\n",
        "        if value not in unique_map.values():\n",
        "            unique_map[key] = value\n",
        "\n",
        "    res = {}\n",
        "    i = 0\n",
        "    for key, value in unique_map.items():\n",
        "        res[i] = value\n",
        "        i += 1\n",
        "    \n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    #if len(sys.argv) != 2:\n",
        "    #    print(\"Command: python main.py <pdf path>\")\n",
        "    #    sys.exit(1)\n",
        "    pdf_path = \"docs/civil-code.pdf\"\n",
        "\n",
        "    starting_words = [\"Titre\", \"Title\", \"Chapitre\", \"Chapter\", \"Section\", \"Sous-section\",\"Sub-section\", \"Paragraphe\",\"Paragraph\" ,\"Article\", \"Livre\", \"Book\" ]\n",
        "    \n",
        "    # Extract sentences with specified starting words\n",
        "    word_sentences = extract_sentences_with_starting_words(pdf_path, starting_words)\n",
        "\n",
        "    l1 = []\n",
        "    for i, word in enumerate(starting_words):\n",
        "        for sentence in word_sentences[i]:\n",
        "            map = (word, sentence)\n",
        "            l1.append(map)\n",
        "\n",
        "    pdf_text = extract_text_by_fontsize(pdf_path)\n",
        "    l2 = process_pdf(pdf_text)\n",
        "\n",
        "    compare = comp(l1, l2)\n",
        "    naksha = make_map(compare)\n",
        "\n",
        "    outfile = open(\"output_uni.json\", \"w\", encoding = 'utf-8') \n",
        "    json.dump(naksha, outfile,indent=2, ensure_ascii=False)\n",
        "    outfile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
